{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\bciez\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product , prediction: 1\n",
      "Text: I hate this product , prediction: 0\n",
      "Text: I like this product , prediction: 1\n",
      "Text: I do not love this product , prediction: 0\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "test_examples = [\n",
    "    {\"text\":'I love this product',\"label\":1},\n",
    "    {\"text\":'I hate this product',\"label\":0},\n",
    "    {\"text\":'I like this product',\"label\":1},\n",
    "    {\"text\":'I do not love this product',\"label\":0}\n",
    "]\n",
    "\n",
    "predictions = sentiment_analysis(\n",
    "    [example['text'] for example in test_examples]\n",
    ")\n",
    "#print(example['text'])\n",
    "\n",
    "true_labels = [example['label'] for example in test_examples]\n",
    "predicted_labels = [1 if pred['label'] ==\"POSITIVE\"\n",
    "                    else 0 for pred in predictions]\n",
    "\n",
    "accuracy= accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# prints results\n",
    "for example, pred_label in zip(test_examples, predicted_labels):\n",
    "    print(f\"Text: {example['text']} , prediction: {pred_label}\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Hugging Metrics --> Metric, Comparison and Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric --> to evaluate model performance based on ground true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate.load('f1').description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.features) # give you info about the inputs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\bciez\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--f1\\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Sat Jun 29 11:00:08 2024) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "print(f1.features) # give you info about the inputs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': Value(dtype='float32', id=None), 'references': Value(dtype='float32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pearson_corr= evaluate.load(\"pearsonr\")\n",
    "print(pearson_corr.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load(\"f1\")\n",
    "real_labels = [0,1,0,1,1]\n",
    "predicted_labels = [0,0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8}\n",
      "{'precision': 1.0}\n",
      "{'recall': 0.6666666666666666}\n",
      "{'f1': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.compute(references=real_labels,predictions=predicted_labels))\n",
    "print(precision.compute(references=real_labels,predictions=predicted_labels))\n",
    "print(recall.compute(references=real_labels,predictions=predicted_labels))\n",
    "print(f1.compute(references=real_labels,predictions=predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM tasks and their metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text classification  \n",
    "\n",
    "--> accuracy, f1 score\n",
    "\n",
    "## text generation \n",
    "--> Perplexity, and BLEU score\n",
    "\n",
    "## Summarization \n",
    "--> ROUGE score and BLEU score\n",
    "\n",
    "## Translation \n",
    "--> BLEU score and METEOR\n",
    "\n",
    "## Q & A \n",
    "--> Exact Match (EM) and BLEU /ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison --> compare two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I do not love this product', 'label': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation --> evaluate and get insight from language datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity ---> text generation\n",
    "\n",
    "model_name = 'gpt2'\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "prompt = \"Lates research findings in antarctica shows\"\n",
    "prompt_ids = tokenizer.encode(test_sentense, return_tensors=\"pt\")\n",
    "output = model.generate(prompt_ids, max_length=17)\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(generated_text)\n",
    "\n",
    "perplexity = evaluate.load('perplexity',module_type=\"metric\")\n",
    "results = perplexity.compute(model_id='gpt2', predictions= generated_text)\n",
    "\n",
    "print(results[mean_perplexity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.22222222222222224, 'rouge2': 0.0, 'rougeL': 0.22222222222222224, 'rougeLsum': 0.22222222222222224}\n"
     ]
    }
   ],
   "source": [
    "# ROUGE score --> text summarization\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "predictions = ['I liked the game']\n",
    "\n",
    "references = ['it was a good game']\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references= references)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation:  What a beautiful day.\n"
     ]
    }
   ],
   "source": [
    "# BLEU score --> translation\n",
    "\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "translator = pipeline('translation',model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "input=\"Que hermoso dia\"\n",
    "references = [[\"what a gergeous day\",\"what a beautiful day\"]]\n",
    "\n",
    "translated_outputs = translator(input)\n",
    "translated_sentence = translated_outputs[0]['translation_text']\n",
    "print('translation: ', translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.6, 0.5, 0.3333333333333333, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.25, 'translation_length': 5, 'reference_length': 4}\n"
     ]
    }
   ],
   "source": [
    "results = bleu.compute(predictions=[translated_sentence], references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bciez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bciez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bciez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu:  0.0\n",
      "Meteor:  0.6084656084656085\n"
     ]
    }
   ],
   "source": [
    "# Meteor Score in translation --> improved alternative to bleu\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "\n",
    "pred = ['what a beautiful day is today']\n",
    "\n",
    "ref =[\"What a nice days\"]\n",
    "\n",
    "results_b = bleu.compute(predictions=pred,references=ref)\n",
    "results_m = meteor.compute(predictions=pred, references=ref)\n",
    "\n",
    "print('bleu: ',results_b['bleu'])\n",
    "print('Meteor: ', results_m['meteor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# Exact Match (EM) --> for questions answering\n",
    "\n",
    "from evaluate  import load\n",
    "em_metric = load('exact_match')\n",
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "predictions = [\"the cat sat on the mat\",\n",
    "               \"theathers are great\",\n",
    "               'it is like comparing oranges and apples']\n",
    "references = ['the cat sat on the mat?',\n",
    "              'theathers are great',\n",
    "              'it is like comparing apples and oranges']\n",
    "\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regard and regard-comparison metrics\n",
    "regard = evaluate.load(\"regard\")\n",
    "#regard_comp = regard(\"regard\", \"compare\")\n",
    "\n",
    "# Compute the regard (polarities) of each group separately\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n",
    "\n",
    "# Compute the relative regard between the two groups for comparison\n",
    "polarity_results_comp = regard.compute(predictions=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
